{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e9c4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split \n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6683e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('GKFlagMarginData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e66560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4d471f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GKFlag</th>\n",
       "      <th>Age</th>\n",
       "      <th>Prof</th>\n",
       "      <th>Det</th>\n",
       "      <th>Amb</th>\n",
       "      <th>Mins</th>\n",
       "      <th>Av Rat</th>\n",
       "      <th>Imp M</th>\n",
       "      <th>Pres</th>\n",
       "      <th>Cons</th>\n",
       "      <th>Ada</th>\n",
       "      <th>Inj Pr</th>\n",
       "      <th>WR</th>\n",
       "      <th>CA</th>\n",
       "      <th>Margin</th>\n",
       "      <th>Training facilities</th>\n",
       "      <th>Div Rep</th>\n",
       "      <th>Growth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34.061602</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "      <td>6.70</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>3000</td>\n",
       "      <td>92</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>107</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>41.494867</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>4380</td>\n",
       "      <td>6.84</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2400</td>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>36.024641</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>540</td>\n",
       "      <td>6.77</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>73</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>35.244353</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>3703</td>\n",
       "      <td>7.21</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>1250</td>\n",
       "      <td>69</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.331964</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>3837</td>\n",
       "      <td>6.56</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2900</td>\n",
       "      <td>103</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>107</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30251</th>\n",
       "      <td>0</td>\n",
       "      <td>16.388775</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>31</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30252</th>\n",
       "      <td>0</td>\n",
       "      <td>16.205339</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30253</th>\n",
       "      <td>0</td>\n",
       "      <td>16.265572</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>107</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30254</th>\n",
       "      <td>0</td>\n",
       "      <td>16.232717</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>36</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30255</th>\n",
       "      <td>0</td>\n",
       "      <td>18.417522</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>825</td>\n",
       "      <td>7.09</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3250</td>\n",
       "      <td>108</td>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>173</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30256 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GKFlag        Age  Prof  Det  Amb  Mins  Av Rat  Imp M  Pres  Cons  \\\n",
       "0           0  34.061602     7    8   11    51    6.70     11     8    10   \n",
       "1           0  41.494867    18   17    8  4380    6.84     14    13     6   \n",
       "2           1  36.024641    12   13   11   540    6.77     13    12    13   \n",
       "3           0  35.244353    18   15   12  3703    7.21     13    16    12   \n",
       "4           0  35.331964    15   16   15  3837    6.56     11    12    12   \n",
       "...       ...        ...   ...  ...  ...   ...     ...    ...   ...   ...   \n",
       "30251       0  16.388775    10   13   10     0    0.00      5    11     7   \n",
       "30252       0  16.205339    14   10    9     0    0.00      4    11     6   \n",
       "30253       0  16.265572    12   13   11     0    0.00      4     8    13   \n",
       "30254       0  16.232717    11    7   13     0    0.00      9    12     6   \n",
       "30255       0  18.417522    14   13   13   825    7.09     12    10    12   \n",
       "\n",
       "       Ada  Inj Pr    WR   CA  Margin  Training facilities  Div Rep  Growth  \n",
       "0       10      14  3000   92      30                   10      107     -11  \n",
       "1       14       1  2400   70      50                    3       46     -10  \n",
       "2        9       5  2000   73      39                    6       85      -6  \n",
       "3       10      14  1250   69      36                    3       46      -4  \n",
       "4       17       6  2900  103      25                   12      107      -8  \n",
       "...    ...     ...   ...  ...     ...                  ...      ...     ...  \n",
       "30251   19      11    50   31      57                    5      107       1  \n",
       "30252    7      12    50   33      60                    5      107       4  \n",
       "30253   14       5    50   30      60                    5      107       6  \n",
       "30254    9       7    50   36      47                    5      107       0  \n",
       "30255    7      10  3250  108      41                   16      173      11  \n",
       "\n",
       "[30256 rows x 18 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3cf6e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['Growth','Ada','WR'], axis=1)\n",
    "y = df['Growth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4da6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split( \n",
    "    x, y, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16db44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "xval, xtest, yval, ytest = train_test_split( \n",
    "    xtest, ytest, test_size=0.5, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f524138",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "xtrain = torch.tensor(xtrain.values)\n",
    "xval = torch.tensor(xval.values)\n",
    "xtest = torch.tensor(xtest.values)\n",
    "ytrain = torch.tensor(ytrain.values)\n",
    "yval = torch.tensor(yval.values)\n",
    "ytest = torch.tensor(ytest.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313fbdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(xtrain, ytrain)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers = 1)\n",
    "\n",
    "val_dataset = TensorDataset(xval, yval)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers = 1)\n",
    "\n",
    "test_dataset = TensorDataset(xtest, ytest)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5914906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(xtrain[0]), 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bound = x[0][-3]\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return torch.min(x[0][0],bound).unsqueeze(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SimpleMLP().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1096f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc6bd5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e86d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs},Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66fae8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best2(9.4flag).pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b062cff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000,Train Loss: 8.3633, Val Loss: 9.2420\n",
      "Epoch 2/1000,Train Loss: 8.3633, Val Loss: 9.2420\n",
      "Epoch 3/1000,Train Loss: 8.3633, Val Loss: 9.2420\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      2\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 4\u001b[0m train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)\u001b[0m\n\u001b[1;32m      9\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat(), labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 23\u001b[0m, in \u001b[0;36mSimpleMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#x = self.dropout(x)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmin(x[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],bound)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "patience = 20\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d53b2493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 8.5849\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ffe041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleMLP(\n",
       "  (fc1): Linear(in_features=15, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08c5cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = xtest.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8cb8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_tensor = torch.unsqueeze(xtest.float(), 0)  # Add batch dimension\n",
    "explainer = shap.DeepExplainer(model, xtest_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adf93d73",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(xtest_tensor)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shap/explainers/_deep/__init__.py:135\u001b[0m, in \u001b[0;36mDeepExplainer.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer\u001b[38;5;241m.\u001b[39mshap_values(X, ranked_outputs, output_rank_order, check_additivity\u001b[38;5;241m=\u001b[39mcheck_additivity)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shap/explainers/_deep/deep_pytorch.py:186\u001b[0m, in \u001b[0;36mPyTorchDeep.shap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# run attribution computation graph\u001b[39;00m\n\u001b[1;32m    185\u001b[0m feature_ind \u001b[38;5;241m=\u001b[39m model_output_ranks[j, i]\n\u001b[0;32m--> 186\u001b[0m sample_phis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient(feature_ind, joint_x)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterim:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shap/explainers/_deep/deep_pytorch.py:119\u001b[0m, in \u001b[0;36mPyTorchDeep.gradient\u001b[0;34m(self, idx, inputs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[0;32m--> 119\u001b[0m         grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(selected, x,\n\u001b[1;32m    120\u001b[0m                                    retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    121\u001b[0m                                    allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m             grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    412\u001b[0m         t_outputs,\n\u001b[1;32m    413\u001b[0m         grad_outputs_,\n\u001b[1;32m    414\u001b[0m         retain_graph,\n\u001b[1;32m    415\u001b[0m         create_graph,\n\u001b[1;32m    416\u001b[0m         inputs,\n\u001b[1;32m    417\u001b[0m         allow_unused,\n\u001b[1;32m    418\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    419\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/hooks.py:138\u001b[0m, in \u001b[0;36mBackwardHook._set_user_hook.<locals>.hook\u001b[0;34m(grad_input, _)\u001b[0m\n\u001b[1;32m    135\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pack_with_none(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_tensors_index, grad_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inputs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_hooks:\n\u001b[0;32m--> 138\u001b[0m     out \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, res, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_outputs)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shap/explainers/_deep/deep_pytorch.py:241\u001b[0m, in \u001b[0;36mdeeplift_grad\u001b[0;34m(module, grad_input, grad_output)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_type \u001b[38;5;129;01min\u001b[39;00m op_handler:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op_handler[module_type]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear_1d\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 241\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m op_handler[module_type](module, grad_input, grad_output)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized nn.Module: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/shap/explainers/_deep/deep_pytorch.py:351\u001b[0m, in \u001b[0;36mnonlinear_1d\u001b[0;34m(module, grad_input, grad_output)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# handles numerical instabilities where delta_in is very small by\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# just taking the gradient in those cases\u001b[39;00m\n\u001b[1;32m    349\u001b[0m grads \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m grad_input]\n\u001b[1;32m    350\u001b[0m grads[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39mabs(delta_in\u001b[38;5;241m.\u001b[39mrepeat(dup0)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m, grad_input[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 351\u001b[0m                        grad_output[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (delta_out \u001b[38;5;241m/\u001b[39m delta_in)\u001b[38;5;241m.\u001b[39mrepeat(dup0))\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(grads)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "shap_values = explainer.shap_values(xtest_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a43a44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([40.2960], device='cuda:0', grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[0,16,20,20,20,4000,7.3,20,20,20,1,100,100,20,185]]).to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9b06cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-11.9398], device='cuda:0', grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[1,35,1,1,1,0,0,1,1,1,20,100,100,1,46]]).to(device).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4678fd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GKFlag</th>\n",
       "      <th>Age</th>\n",
       "      <th>Prof</th>\n",
       "      <th>Det</th>\n",
       "      <th>Amb</th>\n",
       "      <th>Mins</th>\n",
       "      <th>Av Rat</th>\n",
       "      <th>Imp M</th>\n",
       "      <th>Pres</th>\n",
       "      <th>Cons</th>\n",
       "      <th>Ada</th>\n",
       "      <th>Inj Pr</th>\n",
       "      <th>WR</th>\n",
       "      <th>CA</th>\n",
       "      <th>Margin</th>\n",
       "      <th>Training facilities</th>\n",
       "      <th>Div Rep</th>\n",
       "      <th>Growth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34.061602</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "      <td>6.70</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>3000</td>\n",
       "      <td>92</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>107</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>41.494867</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>4380</td>\n",
       "      <td>6.84</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2400</td>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>36.024641</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>540</td>\n",
       "      <td>6.77</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>73</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>35.244353</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>3703</td>\n",
       "      <td>7.21</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>1250</td>\n",
       "      <td>69</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.331964</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>3837</td>\n",
       "      <td>6.56</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2900</td>\n",
       "      <td>103</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>107</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30251</th>\n",
       "      <td>0</td>\n",
       "      <td>16.388775</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>31</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30252</th>\n",
       "      <td>0</td>\n",
       "      <td>16.205339</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30253</th>\n",
       "      <td>0</td>\n",
       "      <td>16.265572</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>107</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30254</th>\n",
       "      <td>0</td>\n",
       "      <td>16.232717</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>36</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30255</th>\n",
       "      <td>0</td>\n",
       "      <td>18.417522</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>825</td>\n",
       "      <td>7.09</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3250</td>\n",
       "      <td>108</td>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>173</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30256 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GKFlag        Age  Prof  Det  Amb  Mins  Av Rat  Imp M  Pres  Cons  \\\n",
       "0           0  34.061602     7    8   11    51    6.70     11     8    10   \n",
       "1           0  41.494867    18   17    8  4380    6.84     14    13     6   \n",
       "2           1  36.024641    12   13   11   540    6.77     13    12    13   \n",
       "3           0  35.244353    18   15   12  3703    7.21     13    16    12   \n",
       "4           0  35.331964    15   16   15  3837    6.56     11    12    12   \n",
       "...       ...        ...   ...  ...  ...   ...     ...    ...   ...   ...   \n",
       "30251       0  16.388775    10   13   10     0    0.00      5    11     7   \n",
       "30252       0  16.205339    14   10    9     0    0.00      4    11     6   \n",
       "30253       0  16.265572    12   13   11     0    0.00      4     8    13   \n",
       "30254       0  16.232717    11    7   13     0    0.00      9    12     6   \n",
       "30255       0  18.417522    14   13   13   825    7.09     12    10    12   \n",
       "\n",
       "       Ada  Inj Pr    WR   CA  Margin  Training facilities  Div Rep  Growth  \n",
       "0       10      14  3000   92      30                   10      107     -11  \n",
       "1       14       1  2400   70      50                    3       46     -10  \n",
       "2        9       5  2000   73      39                    6       85      -6  \n",
       "3       10      14  1250   69      36                    3       46      -4  \n",
       "4       17       6  2900  103      25                   12      107      -8  \n",
       "...    ...     ...   ...  ...     ...                  ...      ...     ...  \n",
       "30251   19      11    50   31      57                    5      107       1  \n",
       "30252    7      12    50   33      60                    5      107       4  \n",
       "30253   14       5    50   30      60                    5      107       6  \n",
       "30254    9       7    50   36      47                    5      107       0  \n",
       "30255    7      10  3250  108      41                   16      173      11  \n",
       "\n",
       "[30256 rows x 18 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b28f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
